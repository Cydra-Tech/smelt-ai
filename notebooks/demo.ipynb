{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Smelt AI — Live Demo\n\nInteractive walkthrough of smelt-ai using **Google Gemini**.\n\n1. Test run — quick single-row validation\n2. Basic classification\n3. Sentiment analysis with score validation\n4. Support ticket triage (complex schema)\n5. Parameter tuning (temperature)\n6. Batch configuration (batch_size, concurrency)\n7. Error handling (stop_on_exhaustion)\n8. Company summaries"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport csv\nfrom pathlib import Path\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\n\nfrom smelt import Model, Job, SmeltResult\nfrom smelt.errors import SmeltExhaustionError\n\nload_dotenv()\n\nGEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\nprint(f\"Gemini key: {'set' if GEMINI_KEY else 'MISSING'}\")\n\nmodel = Model(provider=\"google_genai\", name=\"gemini-3-flash-preview\", api_key=GEMINI_KEY)"
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../tests/data\")\n",
    "\n",
    "\n",
    "def load_csv(filename: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Load CSV from tests/data directory.\"\"\"\n",
    "    with open(DATA_DIR / filename, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "\n",
    "companies = load_csv(\"companies.csv\")\n",
    "products = load_csv(\"products.csv\")\n",
    "tickets = load_csv(\"support_tickets.csv\")\n",
    "\n",
    "print(f\"Companies: {len(companies)} rows\")\n",
    "print(f\"Products:  {len(products)} rows\")\n",
    "print(f\"Tickets:   {len(tickets)} rows\")\n",
    "print()\n",
    "print(\"Sample company:\", companies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Define Output Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "output-models",
   "metadata": {},
   "outputs": [],
   "source": "class IndustryClassification(BaseModel):\n    sector: str = Field(description=\"Primary industry sector\")\n    sub_sector: str = Field(description=\"More specific sub-sector\")\n    is_public: bool = Field(description=\"Whether the company is publicly traded\")\n\n\nclass SentimentAnalysis(BaseModel):\n    sentiment: Literal[\"positive\", \"negative\", \"mixed\"] = Field(description=\"Overall sentiment\")\n    score: float = Field(description=\"Score from 0.0 (negative) to 1.0 (positive)\")\n    key_themes: list[str] = Field(description=\"Main themes in the review (1-3 items)\")\n\n\nclass TicketTriage(BaseModel):\n    category: str = Field(description=\"Category: billing, technical, shipping, account, or general\")\n    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"] = Field(description=\"Priority level\")\n    requires_human: bool = Field(description=\"Whether human escalation is needed\")\n    suggested_response: str = Field(description=\"Brief suggested response to the customer\")\n\n\nclass CompanySummary(BaseModel):\n    one_liner: str = Field(description=\"One sentence description\")\n    industry: str = Field(description=\"Primary industry\")\n    company_size: Literal[\"startup\", \"small\", \"medium\", \"large\", \"enterprise\"] = Field(\n        description=\"Size classification based on employee count\"\n    )\n    age_years: int = Field(description=\"Approximate age in years\")\n\n\nprint(\"Output models defined.\")"
  },
  {
   "cell_type": "markdown",
   "id": "helper-header",
   "metadata": {},
   "source": [
    "## Helper: Pretty-Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(label: str, result: SmeltResult) -> None:\n",
    "    \"\"\"Pretty-print a SmeltResult.\"\"\"\n",
    "    status = \"SUCCESS\" if result.success else \"FAILED\"\n",
    "    m = result.metrics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Rows: {m.successful_rows}/{m.total_rows} successful\")\n",
    "    print(f\"  Batches: {m.successful_batches}/{m.total_batches} successful\")\n",
    "    print(f\"  Tokens: {m.input_tokens:,} in / {m.output_tokens:,} out\")\n",
    "    print(f\"  Retries: {m.total_retries} | Time: {m.wall_time_seconds:.2f}s\")\n",
    "    if result.errors:\n",
    "        print(f\"  Errors: {len(result.errors)}\")\n",
    "        for e in result.errors:\n",
    "            print(f\"    - Batch {e.batch_index}: {e.error_type} ({e.attempts} attempts)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print()\n",
    "    for i, row in enumerate(result.data):\n",
    "        print(f\"  [{i}] {row}\")\n",
    "    if len(result.data) > 3:\n",
    "        print(f\"  ... and {len(result.data) - 3} more rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test1-header",
   "metadata": {},
   "source": "---\n## 1. Test Run — Quick Single-Row Validation\n\nUse `job.atest()` to validate your setup before a full run. Sends only the first row — ignores batch_size, concurrency, and shuffle settings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test1-openai",
   "metadata": {},
   "outputs": [],
   "source": "job = Job(\n    prompt=\"Classify each company by its primary industry sector and sub-sector. \"\n    \"Determine if the company is publicly traded.\",\n    output_model=IndustryClassification,\n    batch_size=20,\n    concurrency=5,\n    shuffle=True,\n)\n\n# Quick test — only the first row is sent, batch_size/concurrency/shuffle are ignored\nresult = await job.atest(model, data=companies)\nshow_result(\"Test Run — Single Row Classification\", result)"
  },
  {
   "cell_type": "markdown",
   "id": "test2-header",
   "metadata": {},
   "source": "---\n## 2. Basic Classification\n\nClassify all 10 companies by industry sector."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test2-openai",
   "metadata": {},
   "outputs": [],
   "source": "job = Job(\n    prompt=\"Classify each company by its primary industry sector and sub-sector. \"\n    \"Determine if the company is publicly traded.\",\n    output_model=IndustryClassification,\n    batch_size=10,\n    stop_on_exhaustion=False,\n)\n\nresult = await job.arun(model, data=companies)\nshow_result(\"Company Classification\", result)"
  },
  {
   "cell_type": "markdown",
   "id": "test3-header",
   "metadata": {},
   "source": "---\n## 3. Sentiment Analysis — Score Validation\n\nAnalyze product reviews and verify scores are in [0, 1] range."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test3-openai",
   "metadata": {},
   "outputs": [],
   "source": "job = Job(\n    prompt=\"Analyze the sentiment of each product's customer_review. \"\n    \"Identify the overall sentiment, assign a score between 0.0 and 1.0, \"\n    \"and extract 1-3 key themes.\",\n    output_model=SentimentAnalysis,\n    batch_size=5,\n    concurrency=2,\n    stop_on_exhaustion=False,\n)\n\nresult = await job.arun(model, data=products)\nshow_result(\"Sentiment Analysis\", result)\n\nprint(\"\\nScore validation:\")\nfor i, row in enumerate(result.data):\n    in_range = 0.0 <= row.score <= 1.0\n    print(f\"  [{i}] score={row.score:.2f} sentiment={row.sentiment:8s} valid={in_range} themes={row.key_themes}\")"
  },
  {
   "cell_type": "markdown",
   "id": "test4-header",
   "metadata": {},
   "source": "---\n## 4. Support Ticket Triage — Complex Schema\n\nTests Literal types, booleans, and longer text generation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test4-temp",
   "metadata": {},
   "outputs": [],
   "source": "job = Job(\n    prompt=\"Triage each support ticket. Classify by category (billing, technical, \"\n    \"shipping, account, or general), assign priority, determine if human escalation \"\n    \"is needed, and write a brief suggested response.\",\n    output_model=TicketTriage,\n    batch_size=5,\n    concurrency=2,\n    stop_on_exhaustion=False,\n)\n\nresult = await job.arun(model, data=tickets)\nshow_result(\"Ticket Triage\", result)\n\nprint(\"\\nFull triage results:\")\nfor i, row in enumerate(result.data):\n    print(f\"\\n  [{i}] {tickets[i]['ticket_id']}\")\n    print(f\"      Category: {row.category} | Priority: {row.priority} | Human: {row.requires_human}\")\n    print(f\"      Response: {row.suggested_response[:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "id": "test5-header",
   "metadata": {},
   "source": "---\n## 5. Parameter Tuning — Temperature Comparison\n\nCompare temperature=0 (deterministic) vs temperature=1.0 (creative) on the same task."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test5-batch",
   "metadata": {},
   "outputs": [],
   "source": "data_subset = companies[:3]\n\nfor temp in [0, 0.5, 1.0]:\n    m = Model(\n        provider=\"google_genai\", name=\"gemini-3-flash-preview\", api_key=GEMINI_KEY,\n        params={\"temperature\": temp},\n    )\n    job = Job(\n        prompt=\"Classify each company by industry sector.\",\n        output_model=IndustryClassification,\n        batch_size=10,\n        stop_on_exhaustion=False,\n    )\n    result = await job.arun(m, data=data_subset)\n    show_result(f\"temp={temp}\", result)"
  },
  {
   "cell_type": "markdown",
   "id": "test6-header",
   "metadata": {},
   "source": "---\n## 6. Batch Configuration — Size & Concurrency\n\nCompare different batch_size and concurrency settings on the same dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test6-collect",
   "metadata": {},
   "outputs": [],
   "source": "configs = [\n    {\"batch_size\": 10, \"concurrency\": 1, \"label\": \"1 batch, serial\"},\n    {\"batch_size\": 5, \"concurrency\": 2, \"label\": \"2 batches, conc=2\"},\n    {\"batch_size\": 2, \"concurrency\": 5, \"label\": \"5 batches, conc=5\"},\n    {\"batch_size\": 1, \"concurrency\": 10, \"label\": \"10 batches, conc=10\"},\n]\n\nfor cfg in configs:\n    job = Job(\n        prompt=\"Classify each company by industry sector.\",\n        output_model=IndustryClassification,\n        batch_size=cfg[\"batch_size\"],\n        concurrency=cfg[\"concurrency\"],\n        stop_on_exhaustion=False,\n    )\n    result = await job.arun(model, data=companies)\n    show_result(f\"Config: {cfg['label']} (batch={cfg['batch_size']}, conc={cfg['concurrency']})\", result)\n\n    assert len(result.data) == len(companies), f\"Row count mismatch\"\n    print(f\"  Row ordering verified: {len(result.data)} rows in correct order\")"
  },
  {
   "cell_type": "markdown",
   "id": "test7-header",
   "metadata": {},
   "source": "---\n## 7. Error Handling — stop_on_exhaustion\n\nDemonstrate `stop_on_exhaustion=True` (raises on failure) vs `stop_on_exhaustion=False` (collects errors)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test7-async-openai",
   "metadata": {},
   "outputs": [],
   "source": "# stop_on_exhaustion=False: errors are collected, successful batches still returned\njob = Job(\n    prompt=\"Create a concise structured summary for each company. \"\n    \"Calculate age based on founded year (current year is 2026).\",\n    output_model=CompanySummary,\n    batch_size=5,\n    concurrency=2,\n    max_retries=2,\n    stop_on_exhaustion=False,\n)\n\nresult = await job.arun(model, data=companies)\nshow_result(\"Company Summary (stop_on_exhaustion=False)\", result)\n\nprint(f\"\\nsuccess property: {result.success}\")\nprint(f\"result.data has {len(result.data)} rows\")\nprint(f\"result.errors has {len(result.errors)} errors\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test7-async-anthropic",
   "metadata": {},
   "outputs": [],
   "source": "# stop_on_exhaustion=True — should succeed without raising\njob = Job(\n    prompt=\"Classify each company by industry sector.\",\n    output_model=IndustryClassification,\n    batch_size=10,\n    max_retries=3,\n    stop_on_exhaustion=True,\n)\n\ntry:\n    result = await job.arun(model, data=companies)\n    show_result(\"Classification (stop_on_exhaustion=True)\", result)\n    print(\"No exception raised — all batches succeeded.\")\nexcept SmeltExhaustionError as e:\n    print(f\"SmeltExhaustionError: {e}\")\n    print(f\"Partial results: {len(e.partial_result.data)} rows succeeded\")\n    print(f\"Errors: {len(e.partial_result.errors)} batches failed\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": "---\n## 8. Company Summaries — Full Dataset\n\nFull run with multi-batch concurrency and detailed metrics."
  },
  {
   "cell_type": "code",
   "id": "889qqg82567",
   "metadata": {},
   "source": "job = Job(\n    prompt=\"Create a concise structured summary for each company. \"\n    \"Calculate the approximate age based on the founded year (current year is 2026).\",\n    output_model=CompanySummary,\n    batch_size=4,\n    concurrency=3,\n    stop_on_exhaustion=False,\n)\n\nresult = await job.arun(model, data=companies)\nshow_result(\"Company Summaries (batch=4, conc=3)\", result)\n\nprint(f\"\\nMetrics breakdown:\")\nprint(f\"  Total batches: {result.metrics.total_batches}\")\nprint(f\"  Input tokens:  {result.metrics.input_tokens:,}\")\nprint(f\"  Output tokens: {result.metrics.output_tokens:,}\")\nprint(f\"  Wall time:     {result.metrics.wall_time_seconds:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "plbxl0g9yf",
   "source": "---\n## Summary\n\nAll tests complete. Smelt successfully:\n- Validates setup with a quick single-row test (`job.atest()`)\n- Transforms structured data through Gemini 3 Flash\n- Returns strictly typed Pydantic models\n- Handles batching and concurrency\n- Provides detailed metrics (tokens, timing, retries)\n- Gracefully handles errors with `stop_on_exhaustion`\n\n> **Note:** Jupyter notebooks run inside an event loop, so all cells use `await job.arun()` / `await job.atest()`.\n> Use `job.run()` / `job.test()` in regular Python scripts.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}